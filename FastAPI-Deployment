we use xgboost as the model.

we do data preprocessing - 
1. loading the dataset
2. removing duplicates 
3. handling missing values
4. creating a new feature
5. Encoding categorical features using label encoder

Accuracies - 
1. Accuracy
2. Precision
3. Recall
4. f1-Score

Data.py -> function to preproces the data and then we also use 
apply_label() to convert the output string into readable output eg- >50k or <50k

And only 'inference' is passed as an object because this helps seperate it from training.

def apply_label(inference):
    """ Convert the binary label in a single inference sample into string output."""
    if inference[0] == 1:
        return ">50K"
    elif inference[0] == 0:
        return "<=50K"
(used to for the output to convert the string output back to our data like under/over 50k)

fbeta = f1_score(y, preds, zero_division=1)
precision = precision_score(y, preds, zero_division=1)
recall = recall_score(y, preds, zero_division=1)

By default, scikit-learn raises a warning or returns 0 if this happens.
zero_division=1 → If division by zero occurs, the metric is set to 1.
zero_division=0 → If division by zero occurs, the metric is set to 0.

The scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1]) 
this is used in XGBClassifer to tells the classifer to consider this imbalnce, for the positive or negative class
basically , this helps us to estimate the postive class properly.

Inference here is a prediction made by the model, 
y is the seperate clas with only labels,
x is the class without the labels and the rest columns

















